{"version":3,"file":"ai-model/service-caller/index.mjs","sources":["webpack://@midscene/core/./src/ai-model/service-caller/index.ts"],"sourcesContent":["import { AIResponseFormat, type AIUsageInfo } from '@/types';\nimport type { CodeGenerationChunk, StreamingCallback } from '@/types';\nimport { Anthropic } from '@anthropic-ai/sdk';\nimport {\n  DefaultAzureCredential,\n  getBearerTokenProvider,\n} from '@azure/identity';\nimport {\n  type IModelPreferences,\n  MIDSCENE_API_TYPE,\n  MIDSCENE_LANGSMITH_DEBUG,\n  OPENAI_MAX_TOKENS,\n  globalConfigManager,\n  uiTarsModelVersion,\n  vlLocateMode,\n} from '@midscene/shared/env';\nimport { parseBase64 } from '@midscene/shared/img';\nimport { getDebug } from '@midscene/shared/logger';\nimport { assert } from '@midscene/shared/utils';\nimport { ifInBrowser } from '@midscene/shared/utils';\nimport { HttpsProxyAgent } from 'https-proxy-agent';\nimport { jsonrepair } from 'jsonrepair';\nimport OpenAI, { AzureOpenAI } from 'openai';\nimport type { ChatCompletionMessageParam } from 'openai/resources/index';\nimport type { Stream } from 'openai/streaming';\nimport { SocksProxyAgent } from 'socks-proxy-agent';\nimport { AIActionType, type AIArgs } from '../common';\nimport { assertSchema } from '../prompt/assertion';\nimport { locatorSchema } from '../prompt/llm-locator';\nimport { planSchema } from '../prompt/llm-planning';\n\nasync function createChatClient({\n  AIActionTypeValue,\n  modelPreferences,\n}: {\n  AIActionTypeValue: AIActionType;\n  modelPreferences: IModelPreferences;\n}): Promise<{\n  completion: OpenAI.Chat.Completions;\n  style: 'openai' | 'anthropic';\n  modelName: string;\n  modelDescription: string;\n}> {\n  const {\n    socksProxy,\n    httpProxy,\n    modelName,\n    openaiBaseURL,\n    openaiApiKey,\n    openaiExtraConfig,\n    openaiUseAzureDeprecated,\n    useAzureOpenai,\n    azureOpenaiScope,\n    azureOpenaiKey,\n    azureOpenaiEndpoint,\n    azureOpenaiApiVersion,\n    azureOpenaiDeployment,\n    azureExtraConfig,\n    useAnthropicSdk,\n    anthropicApiKey,\n    modelDescription,\n  } = globalConfigManager.getModelConfigByIntent(modelPreferences.intent);\n\n  let openai: OpenAI | AzureOpenAI | undefined;\n\n  let proxyAgent = undefined;\n  const debugProxy = getDebug('ai:call:proxy');\n  if (httpProxy) {\n    debugProxy('using http proxy', httpProxy);\n    proxyAgent = new HttpsProxyAgent(httpProxy);\n  } else if (socksProxy) {\n    debugProxy('using socks proxy', socksProxy);\n    proxyAgent = new SocksProxyAgent(socksProxy);\n  }\n\n  if (openaiUseAzureDeprecated) {\n    // this is deprecated\n    openai = new AzureOpenAI({\n      baseURL: openaiBaseURL,\n      apiKey: openaiApiKey,\n      httpAgent: proxyAgent,\n      ...openaiExtraConfig,\n      dangerouslyAllowBrowser: true,\n    }) as OpenAI;\n  } else if (useAzureOpenai) {\n    // https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=bash%2Cjavascript-key%2Ctypescript-keyless%2Cpython&pivots=programming-language-javascript#rest-api\n    // keyless authentication\n    let tokenProvider: any = undefined;\n    if (azureOpenaiScope) {\n      assert(\n        !ifInBrowser,\n        'Azure OpenAI is not supported in browser with Midscene.',\n      );\n      const credential = new DefaultAzureCredential();\n\n      tokenProvider = getBearerTokenProvider(credential, azureOpenaiScope);\n\n      openai = new AzureOpenAI({\n        azureADTokenProvider: tokenProvider,\n        endpoint: azureOpenaiEndpoint,\n        apiVersion: azureOpenaiApiVersion,\n        deployment: azureOpenaiDeployment,\n        ...openaiExtraConfig,\n        ...azureExtraConfig,\n      });\n    } else {\n      // endpoint, apiKey, apiVersion, deployment\n      openai = new AzureOpenAI({\n        apiKey: azureOpenaiKey,\n        endpoint: azureOpenaiEndpoint,\n        apiVersion: azureOpenaiApiVersion,\n        deployment: azureOpenaiDeployment,\n        dangerouslyAllowBrowser: true,\n        ...openaiExtraConfig,\n        ...azureExtraConfig,\n      });\n    }\n  } else if (!useAnthropicSdk) {\n    openai = new OpenAI({\n      baseURL: openaiBaseURL,\n      apiKey: openaiApiKey,\n      httpAgent: proxyAgent,\n      ...openaiExtraConfig,\n      defaultHeaders: {\n        ...(openaiExtraConfig?.defaultHeaders || {}),\n        [MIDSCENE_API_TYPE]: AIActionTypeValue.toString(),\n      },\n      dangerouslyAllowBrowser: true,\n    });\n  }\n\n  if (\n    openai &&\n    globalConfigManager.getEnvConfigInBoolean(MIDSCENE_LANGSMITH_DEBUG)\n  ) {\n    if (ifInBrowser) {\n      throw new Error('langsmith is not supported in browser');\n    }\n    console.log('DEBUGGING MODE: langsmith wrapper enabled');\n    const { wrapOpenAI } = await import('langsmith/wrappers');\n    openai = wrapOpenAI(openai);\n  }\n\n  if (typeof openai !== 'undefined') {\n    return {\n      completion: openai.chat.completions,\n      style: 'openai',\n      modelName,\n      modelDescription,\n    };\n  }\n\n  // Anthropic\n  if (useAnthropicSdk) {\n    openai = new Anthropic({\n      apiKey: anthropicApiKey,\n      httpAgent: proxyAgent,\n      dangerouslyAllowBrowser: true,\n    }) as any;\n  }\n\n  if (typeof openai !== 'undefined' && (openai as any).messages) {\n    return {\n      completion: (openai as any).messages,\n      style: 'anthropic',\n      modelName,\n      modelDescription,\n    };\n  }\n\n  throw new Error('Openai SDK or Anthropic SDK is not initialized');\n}\n\nexport async function call(\n  messages: ChatCompletionMessageParam[],\n  AIActionTypeValue: AIActionType,\n  modelPreferences: IModelPreferences,\n  options?: {\n    stream?: boolean;\n    onChunk?: StreamingCallback;\n  },\n): Promise<{ content: string; usage?: AIUsageInfo; isStreamed: boolean }> {\n  const { completion, style, modelName, modelDescription } =\n    await createChatClient({\n      AIActionTypeValue,\n      modelPreferences,\n    });\n\n  const responseFormat = getResponseFormat(modelName, AIActionTypeValue);\n\n  const maxTokens = globalConfigManager.getEnvConfigValue(OPENAI_MAX_TOKENS);\n  const debugCall = getDebug('ai:call');\n  const debugProfileStats = getDebug('ai:profile:stats');\n  const debugProfileDetail = getDebug('ai:profile:detail');\n\n  const startTime = Date.now();\n\n  const isStreaming = options?.stream && options?.onChunk;\n  let content: string | undefined;\n  let accumulated = '';\n  let usage: OpenAI.CompletionUsage | undefined;\n  let timeCost: number | undefined;\n\n  const commonConfig = {\n    temperature: vlLocateMode(modelPreferences) === 'vlm-ui-tars' ? 0.0 : 0.1,\n    stream: !!isStreaming,\n    max_tokens:\n      typeof maxTokens === 'number'\n        ? maxTokens\n        : Number.parseInt(maxTokens || '2048', 10),\n    ...(vlLocateMode(modelPreferences) === 'qwen-vl' // qwen specific config\n      ? {\n          vl_high_resolution_images: true,\n        }\n      : {}),\n  };\n\n  try {\n    if (style === 'openai') {\n      debugCall(\n        `sending ${isStreaming ? 'streaming ' : ''}request to ${modelName}`,\n      );\n\n      if (isStreaming) {\n        const stream = (await completion.create(\n          {\n            model: modelName,\n            messages,\n            response_format: responseFormat,\n            ...commonConfig,\n          },\n          {\n            stream: true,\n          },\n        )) as Stream<OpenAI.Chat.Completions.ChatCompletionChunk> & {\n          _request_id?: string | null;\n        };\n\n        for await (const chunk of stream) {\n          const content = chunk.choices?.[0]?.delta?.content || '';\n          const reasoning_content =\n            (chunk.choices?.[0]?.delta as any)?.reasoning_content || '';\n\n          // Check for usage info in any chunk (OpenAI provides usage in separate chunks)\n          if (chunk.usage) {\n            usage = chunk.usage;\n          }\n\n          if (content || reasoning_content) {\n            accumulated += content;\n            const chunkData: CodeGenerationChunk = {\n              content,\n              reasoning_content,\n              accumulated,\n              isComplete: false,\n              usage: undefined,\n            };\n            options.onChunk!(chunkData);\n          }\n\n          // Check if stream is complete\n          if (chunk.choices?.[0]?.finish_reason) {\n            timeCost = Date.now() - startTime;\n\n            // If usage is not available from the stream, provide a basic usage info\n            if (!usage) {\n              // Estimate token counts based on content length (rough approximation)\n              const estimatedTokens = Math.max(\n                1,\n                Math.floor(accumulated.length / 4),\n              );\n              usage = {\n                prompt_tokens: estimatedTokens,\n                completion_tokens: estimatedTokens,\n                total_tokens: estimatedTokens * 2,\n              };\n            }\n\n            // Send final chunk\n            const finalChunk: CodeGenerationChunk = {\n              content: '',\n              accumulated,\n              reasoning_content: '',\n              isComplete: true,\n              usage: {\n                prompt_tokens: usage.prompt_tokens ?? 0,\n                completion_tokens: usage.completion_tokens ?? 0,\n                total_tokens: usage.total_tokens ?? 0,\n                time_cost: timeCost ?? 0,\n                model_name: modelName,\n                model_description: modelDescription,\n                intent: modelPreferences.intent,\n              },\n            };\n            options.onChunk!(finalChunk);\n            break;\n          }\n        }\n        content = accumulated;\n        debugProfileStats(\n          `streaming model, ${modelName}, mode, ${vlLocateMode(modelPreferences) || 'default'}, cost-ms, ${timeCost}`,\n        );\n      } else {\n        const result = await completion.create({\n          model: modelName,\n          messages,\n          response_format: responseFormat,\n          ...commonConfig,\n        } as any);\n        timeCost = Date.now() - startTime;\n\n        debugProfileStats(\n          `model, ${modelName}, mode, ${vlLocateMode(modelPreferences) || 'default'}, ui-tars-version, ${uiTarsModelVersion(modelPreferences)}, prompt-tokens, ${result.usage?.prompt_tokens || ''}, completion-tokens, ${result.usage?.completion_tokens || ''}, total-tokens, ${result.usage?.total_tokens || ''}, cost-ms, ${timeCost}, requestId, ${result._request_id || ''}`,\n        );\n\n        debugProfileDetail(\n          `model usage detail: ${JSON.stringify(result.usage)}`,\n        );\n\n        assert(\n          result.choices,\n          `invalid response from LLM service: ${JSON.stringify(result)}`,\n        );\n        content = result.choices[0].message.content!;\n        usage = result.usage;\n      }\n\n      debugCall(`response: ${content}`);\n      assert(content, 'empty content');\n    } else if (style === 'anthropic') {\n      const convertImageContent = (content: any) => {\n        if (content.type === 'image_url') {\n          const imgBase64 = content.image_url.url;\n          assert(imgBase64, 'image_url is required');\n          const { mimeType, body } = parseBase64(content.image_url.url);\n          return {\n            source: {\n              type: 'base64',\n              media_type: mimeType,\n              data: body,\n            },\n            type: 'image',\n          };\n        }\n        return content;\n      };\n\n      if (isStreaming) {\n        const stream = (await completion.create({\n          model: modelName,\n          system: 'You are a versatile professional in software UI automation',\n          messages: messages.map((m) => ({\n            role: 'user',\n            content: Array.isArray(m.content)\n              ? (m.content as any).map(convertImageContent)\n              : m.content,\n          })),\n          response_format: responseFormat,\n          ...commonConfig,\n        } as any)) as any;\n\n        for await (const chunk of stream) {\n          const content = chunk.delta?.text || '';\n          if (content) {\n            accumulated += content;\n            const chunkData: CodeGenerationChunk = {\n              content,\n              accumulated,\n              reasoning_content: '',\n              isComplete: false,\n              usage: undefined,\n            };\n            options.onChunk!(chunkData);\n          }\n\n          // Check if stream is complete\n          if (chunk.type === 'message_stop') {\n            timeCost = Date.now() - startTime;\n            const anthropicUsage = chunk.usage;\n\n            // Send final chunk\n            const finalChunk: CodeGenerationChunk = {\n              content: '',\n              accumulated,\n              reasoning_content: '',\n              isComplete: true,\n              usage: anthropicUsage\n                ? {\n                    prompt_tokens: anthropicUsage.input_tokens ?? 0,\n                    completion_tokens: anthropicUsage.output_tokens ?? 0,\n                    total_tokens:\n                      (anthropicUsage.input_tokens ?? 0) +\n                      (anthropicUsage.output_tokens ?? 0),\n                    time_cost: timeCost ?? 0,\n                    model_name: modelName,\n                    model_description: modelDescription,\n                    intent: modelPreferences.intent,\n                  }\n                : undefined,\n            };\n            options.onChunk!(finalChunk);\n            break;\n          }\n        }\n        content = accumulated;\n      } else {\n        const result = await completion.create({\n          model: modelName,\n          system: 'You are a versatile professional in software UI automation',\n          messages: messages.map((m) => ({\n            role: 'user',\n            content: Array.isArray(m.content)\n              ? (m.content as any).map(convertImageContent)\n              : m.content,\n          })),\n          response_format: responseFormat,\n          ...commonConfig,\n        } as any);\n        timeCost = Date.now() - startTime;\n        content = (result as any).content[0].text as string;\n        usage = result.usage;\n      }\n\n      assert(content, 'empty content');\n    }\n    // Ensure we always have usage info for streaming responses\n    if (isStreaming && !usage) {\n      // Estimate token counts based on content length (rough approximation)\n      const estimatedTokens = Math.max(\n        1,\n        Math.floor((content || '').length / 4),\n      );\n      usage = {\n        prompt_tokens: estimatedTokens,\n        completion_tokens: estimatedTokens,\n        total_tokens: estimatedTokens * 2,\n      };\n    }\n\n    return {\n      content: content || '',\n      usage: usage\n        ? {\n            prompt_tokens: usage.prompt_tokens ?? 0,\n            completion_tokens: usage.completion_tokens ?? 0,\n            total_tokens: usage.total_tokens ?? 0,\n            time_cost: timeCost ?? 0,\n            model_name: modelName,\n            model_description: modelDescription,\n            intent: modelPreferences.intent,\n          }\n        : undefined,\n      isStreamed: !!isStreaming,\n    };\n  } catch (e: any) {\n    console.error(' call AI error', e);\n    const newError = new Error(\n      `failed to call ${isStreaming ? 'streaming ' : ''}AI model service: ${e.message}. Trouble shooting: https://midscenejs.com/model-provider.html`,\n      {\n        cause: e,\n      },\n    );\n    throw newError;\n  }\n}\n\nexport const getResponseFormat = (\n  modelName: string,\n  AIActionTypeValue: AIActionType,\n):\n  | OpenAI.ChatCompletionCreateParams['response_format']\n  | OpenAI.ResponseFormatJSONObject => {\n  let responseFormat:\n    | OpenAI.ChatCompletionCreateParams['response_format']\n    | OpenAI.ResponseFormatJSONObject\n    | undefined;\n\n  if (modelName.includes('gpt-4')) {\n    switch (AIActionTypeValue) {\n      case AIActionType.ASSERT:\n        responseFormat = assertSchema;\n        break;\n      case AIActionType.INSPECT_ELEMENT:\n        responseFormat = locatorSchema;\n        break;\n      case AIActionType.PLAN:\n        responseFormat = planSchema;\n        break;\n      case AIActionType.EXTRACT_DATA:\n      case AIActionType.DESCRIBE_ELEMENT:\n        responseFormat = { type: AIResponseFormat.JSON };\n        break;\n    }\n  }\n\n  // gpt-4o-2024-05-13 only supports json_object response format\n  if (modelName === 'gpt-4o-2024-05-13') {\n    responseFormat = { type: AIResponseFormat.JSON };\n  }\n\n  return responseFormat;\n};\n\nexport async function callToGetJSONObject<T>(\n  messages: ChatCompletionMessageParam[],\n  AIActionTypeValue: AIActionType,\n  modelPreferences: IModelPreferences,\n): Promise<{ content: T; usage?: AIUsageInfo }> {\n  const response = await call(messages, AIActionTypeValue, modelPreferences);\n  assert(response, 'empty response');\n  const jsonContent = safeParseJson(response.content, modelPreferences);\n  return { content: jsonContent, usage: response.usage };\n}\n\nexport async function callAiFnWithStringResponse<T>(\n  msgs: AIArgs,\n  AIActionTypeValue: AIActionType,\n  modelPreferences: IModelPreferences,\n): Promise<{ content: string; usage?: AIUsageInfo }> {\n  const { content, usage } = await call(\n    msgs,\n    AIActionTypeValue,\n    modelPreferences,\n  );\n  return { content, usage };\n}\n\nexport function extractJSONFromCodeBlock(response: string) {\n  try {\n    // First, try to match a JSON object directly in the response\n    const jsonMatch = response.match(/^\\s*(\\{[\\s\\S]*\\})\\s*$/);\n    if (jsonMatch) {\n      return jsonMatch[1];\n    }\n\n    // If no direct JSON object is found, try to extract JSON from a code block\n    const codeBlockMatch = response.match(\n      /```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```/,\n    );\n    if (codeBlockMatch) {\n      return codeBlockMatch[1];\n    }\n\n    // If no code block is found, try to find a JSON-like structure in the text\n    const jsonLikeMatch = response.match(/\\{[\\s\\S]*\\}/);\n    if (jsonLikeMatch) {\n      return jsonLikeMatch[0];\n    }\n  } catch {}\n  // If no JSON-like structure is found, return the original response\n  return response;\n}\n\nexport function preprocessDoubaoBboxJson(input: string) {\n  if (input.includes('bbox')) {\n    // when its values like 940 445 969 490, replace all /\\d+\\s+\\d+/g with /$1,$2/g\n    while (/\\d+\\s+\\d+/.test(input)) {\n      input = input.replace(/(\\d+)\\s+(\\d+)/g, '$1,$2');\n    }\n  }\n  return input;\n}\n\nexport function safeParseJson(\n  input: string,\n  modelPreferences: IModelPreferences,\n) {\n  const cleanJsonString = extractJSONFromCodeBlock(input);\n  // match the point\n  if (cleanJsonString?.match(/\\((\\d+),(\\d+)\\)/)) {\n    return cleanJsonString\n      .match(/\\((\\d+),(\\d+)\\)/)\n      ?.slice(1)\n      .map(Number);\n  }\n  try {\n    return JSON.parse(cleanJsonString);\n  } catch {}\n  try {\n    return JSON.parse(jsonrepair(cleanJsonString));\n  } catch (e) {}\n\n  if (\n    vlLocateMode(modelPreferences) === 'doubao-vision' ||\n    vlLocateMode(modelPreferences) === 'vlm-ui-tars'\n  ) {\n    const jsonString = preprocessDoubaoBboxJson(cleanJsonString);\n    return JSON.parse(jsonrepair(jsonString));\n  }\n  throw Error(`failed to parse json response: ${input}`);\n}\n"],"names":["createChatClient","AIActionTypeValue","modelPreferences","socksProxy","httpProxy","modelName","openaiBaseURL","openaiApiKey","openaiExtraConfig","openaiUseAzureDeprecated","useAzureOpenai","azureOpenaiScope","azureOpenaiKey","azureOpenaiEndpoint","azureOpenaiApiVersion","azureOpenaiDeployment","azureExtraConfig","useAnthropicSdk","anthropicApiKey","modelDescription","globalConfigManager","openai","proxyAgent","debugProxy","getDebug","HttpsProxyAgent","SocksProxyAgent","AzureOpenAI","tokenProvider","assert","ifInBrowser","credential","DefaultAzureCredential","getBearerTokenProvider","OpenAI","MIDSCENE_API_TYPE","MIDSCENE_LANGSMITH_DEBUG","Error","console","wrapOpenAI","Anthropic","call","messages","options","completion","style","responseFormat","getResponseFormat","maxTokens","OPENAI_MAX_TOKENS","debugCall","debugProfileStats","debugProfileDetail","startTime","Date","isStreaming","content","accumulated","usage","timeCost","commonConfig","vlLocateMode","Number","stream","chunk","_chunk_choices__delta","_chunk_choices__delta1","_chunk_choices_2","reasoning_content","chunkData","undefined","estimatedTokens","Math","finalChunk","_result_usage","_result_usage1","_result_usage2","result","uiTarsModelVersion","JSON","convertImageContent","imgBase64","mimeType","body","parseBase64","m","Array","_chunk_delta","anthropicUsage","e","newError","AIActionType","assertSchema","locatorSchema","planSchema","AIResponseFormat","callToGetJSONObject","response","jsonContent","safeParseJson","callAiFnWithStringResponse","msgs","extractJSONFromCodeBlock","jsonMatch","codeBlockMatch","jsonLikeMatch","preprocessDoubaoBboxJson","input","cleanJsonString","_cleanJsonString_match","jsonrepair","jsonString"],"mappings":";;;;;;;;;;;;;;;AA+BA,eAAeA,iBAAiB,EAC9BC,iBAAiB,EACjBC,gBAAgB,EAIjB;IAMC,MAAM,EACJC,UAAU,EACVC,SAAS,EACTC,SAAS,EACTC,aAAa,EACbC,YAAY,EACZC,iBAAiB,EACjBC,wBAAwB,EACxBC,cAAc,EACdC,gBAAgB,EAChBC,cAAc,EACdC,mBAAmB,EACnBC,qBAAqB,EACrBC,qBAAqB,EACrBC,gBAAgB,EAChBC,eAAe,EACfC,eAAe,EACfC,gBAAgB,EACjB,GAAGC,oBAAoB,sBAAsB,CAAClB,iBAAiB,MAAM;IAEtE,IAAImB;IAEJ,IAAIC;IACJ,MAAMC,aAAaC,SAAS;IAC5B,IAAIpB,WAAW;QACbmB,WAAW,oBAAoBnB;QAC/BkB,aAAa,IAAIG,gBAAgBrB;IACnC,OAAO,IAAID,YAAY;QACrBoB,WAAW,qBAAqBpB;QAChCmB,aAAa,IAAII,gBAAgBvB;IACnC;IAEA,IAAIM,0BAEFY,SAAS,IAAIM,YAAY;QACvB,SAASrB;QACT,QAAQC;QACR,WAAWe;QACX,GAAGd,iBAAiB;QACpB,yBAAyB;IAC3B;SACK,IAAIE,gBAAgB;QAGzB,IAAIkB;QACJ,IAAIjB,kBAAkB;YACpBkB,OACE,CAACC,aACD;YAEF,MAAMC,aAAa,IAAIC;YAEvBJ,gBAAgBK,uBAAuBF,YAAYpB;YAEnDU,SAAS,IAAIM,YAAY;gBACvB,sBAAsBC;gBACtB,UAAUf;gBACV,YAAYC;gBACZ,YAAYC;gBACZ,GAAGP,iBAAiB;gBACpB,GAAGQ,gBAAgB;YACrB;QACF,OAEEK,SAAS,IAAIM,YAAY;YACvB,QAAQf;YACR,UAAUC;YACV,YAAYC;YACZ,YAAYC;YACZ,yBAAyB;YACzB,GAAGP,iBAAiB;YACpB,GAAGQ,gBAAgB;QACrB;IAEJ,OAAO,IAAI,CAACC,iBACVI,SAAS,IAAIa,SAAO;QAClB,SAAS5B;QACT,QAAQC;QACR,WAAWe;QACX,GAAGd,iBAAiB;QACpB,gBAAgB;YACd,GAAIA,AAAAA,CAAAA,QAAAA,oBAAAA,KAAAA,IAAAA,kBAAmB,cAAc,AAAD,KAAK,CAAC,CAAC;YAC3C,CAAC2B,kBAAkB,EAAElC,kBAAkB,QAAQ;QACjD;QACA,yBAAyB;IAC3B;IAGF,IACEoB,UACAD,oBAAoB,qBAAqB,CAACgB,2BAC1C;QACA,IAAIN,aACF,MAAM,IAAIO,MAAM;QAElBC,QAAQ,GAAG,CAAC;QACZ,MAAM,EAAEC,UAAU,EAAE,GAAG,MAAM,MAAM,CAAC;QACpClB,SAASkB,WAAWlB;IACtB;IAEA,IAAI,AAAkB,WAAXA,QACT,OAAO;QACL,YAAYA,OAAO,IAAI,CAAC,WAAW;QACnC,OAAO;QACPhB;QACAc;IACF;IAIF,IAAIF,iBACFI,SAAS,IAAImB,UAAU;QACrB,QAAQtB;QACR,WAAWI;QACX,yBAAyB;IAC3B;IAGF,IAAI,AAAkB,WAAXD,UAA2BA,OAAe,QAAQ,EAC3D,OAAO;QACL,YAAaA,OAAe,QAAQ;QACpC,OAAO;QACPhB;QACAc;IACF;IAGF,MAAM,IAAIkB,MAAM;AAClB;AAEO,eAAeI,KACpBC,QAAsC,EACtCzC,iBAA+B,EAC/BC,gBAAmC,EACnCyC,OAGC;IAED,MAAM,EAAEC,UAAU,EAAEC,KAAK,EAAExC,SAAS,EAAEc,gBAAgB,EAAE,GACtD,MAAMnB,iBAAiB;QACrBC;QACAC;IACF;IAEF,MAAM4C,iBAAiBC,kBAAkB1C,WAAWJ;IAEpD,MAAM+C,YAAY5B,oBAAoB,iBAAiB,CAAC6B;IACxD,MAAMC,YAAY1B,SAAS;IAC3B,MAAM2B,oBAAoB3B,SAAS;IACnC,MAAM4B,qBAAqB5B,SAAS;IAEpC,MAAM6B,YAAYC,KAAK,GAAG;IAE1B,MAAMC,cAAcZ,AAAAA,CAAAA,QAAAA,UAAAA,KAAAA,IAAAA,QAAS,MAAM,AAAD,KAAKA,CAAAA,QAAAA,UAAAA,KAAAA,IAAAA,QAAS,OAAO,AAAD;IACtD,IAAIa;IACJ,IAAIC,cAAc;IAClB,IAAIC;IACJ,IAAIC;IAEJ,MAAMC,eAAe;QACnB,aAAaC,AAAmC,kBAAnCA,aAAa3D,oBAAsC,MAAM;QACtE,QAAQ,CAAC,CAACqD;QACV,YACE,AAAqB,YAArB,OAAOP,YACHA,YACAc,OAAO,QAAQ,CAACd,aAAa,QAAQ;QAC3C,GAAIa,AAAmC,cAAnCA,aAAa3D,oBACb;YACE,2BAA2B;QAC7B,IACA,CAAC,CAAC;IACR;IAEA,IAAI;QACF,IAAI2C,AAAU,aAAVA,OAAoB;YACtBK,UACE,CAAC,QAAQ,EAAEK,cAAc,eAAe,GAAG,WAAW,EAAElD,WAAW;YAGrE,IAAIkD,aAAa;gBACf,MAAMQ,SAAU,MAAMnB,WAAW,MAAM,CACrC;oBACE,OAAOvC;oBACPqC;oBACA,iBAAiBI;oBACjB,GAAGc,YAAY;gBACjB,GACA;oBACE,QAAQ;gBACV;gBAKF,WAAW,MAAMI,SAASD,OAAQ;wBAChBE,uBAAAA,iBAAAA,gBAEbC,wBAAAA,kBAAAA,iBAoBCC,kBAAAA;oBAtBJ,MAAMX,UAAUS,AAAAA,SAAAA,CAAAA,iBAAAA,MAAM,OAAO,AAAD,IAAZA,KAAAA,IAAAA,QAAAA,CAAAA,kBAAAA,cAAe,CAAC,EAAE,AAAD,IAAjBA,KAAAA,IAAAA,QAAAA,CAAAA,wBAAAA,gBAAoB,KAAK,AAAD,IAAxBA,KAAAA,IAAAA,sBAA2B,OAAO,AAAD,KAAK;oBACtD,MAAMG,oBACJ,AAAC,SAAAF,CAAAA,kBAAAA,MAAM,OAAO,AAAD,IAAZA,KAAAA,IAAAA,QAAAA,CAAAA,mBAAAA,eAAe,CAAC,EAAE,AAAD,IAAjBA,KAAAA,IAAAA,QAAAA,CAAAA,yBAAAA,iBAAoB,KAAK,AAAD,IAAxBA,KAAAA,IAAAA,uBAAmC,iBAAiB,AAAD,KAAK;oBAG3D,IAAIF,MAAM,KAAK,EACbN,QAAQM,MAAM,KAAK;oBAGrB,IAAIR,WAAWY,mBAAmB;wBAChCX,eAAeD;wBACf,MAAMa,YAAiC;4BACrCb;4BACAY;4BACAX;4BACA,YAAY;4BACZ,OAAOa;wBACT;wBACA3B,QAAQ,OAAO,CAAE0B;oBACnB;oBAGA,IAAI,QAAAF,CAAAA,kBAAAA,MAAM,OAAO,AAAD,IAAZA,KAAAA,IAAAA,QAAAA,CAAAA,mBAAAA,eAAe,CAAC,EAAE,AAAD,IAAjBA,KAAAA,IAAAA,iBAAoB,aAAa,EAAE;wBACrCR,WAAWL,KAAK,GAAG,KAAKD;wBAGxB,IAAI,CAACK,OAAO;4BAEV,MAAMa,kBAAkBC,KAAK,GAAG,CAC9B,GACAA,KAAK,KAAK,CAACf,YAAY,MAAM,GAAG;4BAElCC,QAAQ;gCACN,eAAea;gCACf,mBAAmBA;gCACnB,cAAcA,AAAkB,IAAlBA;4BAChB;wBACF;wBAGA,MAAME,aAAkC;4BACtC,SAAS;4BACThB;4BACA,mBAAmB;4BACnB,YAAY;4BACZ,OAAO;gCACL,eAAeC,MAAM,aAAa,IAAI;gCACtC,mBAAmBA,MAAM,iBAAiB,IAAI;gCAC9C,cAAcA,MAAM,YAAY,IAAI;gCACpC,WAAWC,YAAY;gCACvB,YAAYtD;gCACZ,mBAAmBc;gCACnB,QAAQjB,iBAAiB,MAAM;4BACjC;wBACF;wBACAyC,QAAQ,OAAO,CAAE8B;wBACjB;oBACF;gBACF;gBACAjB,UAAUC;gBACVN,kBACE,CAAC,iBAAiB,EAAE9C,UAAU,QAAQ,EAAEwD,aAAa3D,qBAAqB,UAAU,WAAW,EAAEyD,UAAU;YAE/G,OAAO;oBAUoJe,eAAyDC,gBAAwDC;gBAT1Q,MAAMC,SAAS,MAAMjC,WAAW,MAAM,CAAC;oBACrC,OAAOvC;oBACPqC;oBACA,iBAAiBI;oBACjB,GAAGc,YAAY;gBACjB;gBACAD,WAAWL,KAAK,GAAG,KAAKD;gBAExBF,kBACE,CAAC,OAAO,EAAE9C,UAAU,QAAQ,EAAEwD,aAAa3D,qBAAqB,UAAU,mBAAmB,EAAE4E,mBAAmB5E,kBAAkB,iBAAiB,EAAEwE,AAAAA,SAAAA,CAAAA,gBAAAA,OAAO,KAAK,AAAD,IAAXA,KAAAA,IAAAA,cAAc,aAAa,AAAD,KAAK,GAAG,qBAAqB,EAAEC,AAAAA,SAAAA,CAAAA,iBAAAA,OAAO,KAAK,AAAD,IAAXA,KAAAA,IAAAA,eAAc,iBAAiB,AAAD,KAAK,GAAG,gBAAgB,EAAEC,AAAAA,SAAAA,CAAAA,iBAAAA,OAAO,KAAK,AAAD,IAAXA,KAAAA,IAAAA,eAAc,YAAY,AAAD,KAAK,GAAG,WAAW,EAAEjB,SAAS,aAAa,EAAEkB,OAAO,WAAW,IAAI,IAAI;gBAG1WzB,mBACE,CAAC,oBAAoB,EAAE2B,KAAK,SAAS,CAACF,OAAO,KAAK,GAAG;gBAGvDhD,OACEgD,OAAO,OAAO,EACd,CAAC,mCAAmC,EAAEE,KAAK,SAAS,CAACF,SAAS;gBAEhErB,UAAUqB,OAAO,OAAO,CAAC,EAAE,CAAC,OAAO,CAAC,OAAO;gBAC3CnB,QAAQmB,OAAO,KAAK;YACtB;YAEA3B,UAAU,CAAC,UAAU,EAAEM,SAAS;YAChC3B,OAAO2B,SAAS;QAClB,OAAO,IAAIX,AAAU,gBAAVA,OAAuB;YAChC,MAAMmC,sBAAsB,CAACxB;gBAC3B,IAAIA,AAAiB,gBAAjBA,QAAQ,IAAI,EAAkB;oBAChC,MAAMyB,YAAYzB,QAAQ,SAAS,CAAC,GAAG;oBACvC3B,OAAOoD,WAAW;oBAClB,MAAM,EAAEC,QAAQ,EAAEC,IAAI,EAAE,GAAGC,YAAY5B,QAAQ,SAAS,CAAC,GAAG;oBAC5D,OAAO;wBACL,QAAQ;4BACN,MAAM;4BACN,YAAY0B;4BACZ,MAAMC;wBACR;wBACA,MAAM;oBACR;gBACF;gBACA,OAAO3B;YACT;YAEA,IAAID,aAAa;gBACf,MAAMQ,SAAU,MAAMnB,WAAW,MAAM,CAAC;oBACtC,OAAOvC;oBACP,QAAQ;oBACR,UAAUqC,SAAS,GAAG,CAAC,CAAC2C,IAAO;4BAC7B,MAAM;4BACN,SAASC,MAAM,OAAO,CAACD,EAAE,OAAO,IAC3BA,EAAE,OAAO,CAAS,GAAG,CAACL,uBACvBK,EAAE,OAAO;wBACf;oBACA,iBAAiBvC;oBACjB,GAAGc,YAAY;gBACjB;gBAEA,WAAW,MAAMI,SAASD,OAAQ;wBAChBwB;oBAAhB,MAAM/B,UAAU+B,AAAAA,SAAAA,CAAAA,eAAAA,MAAM,KAAK,AAAD,IAAVA,KAAAA,IAAAA,aAAa,IAAI,AAAD,KAAK;oBACrC,IAAI/B,SAAS;wBACXC,eAAeD;wBACf,MAAMa,YAAiC;4BACrCb;4BACAC;4BACA,mBAAmB;4BACnB,YAAY;4BACZ,OAAOa;wBACT;wBACA3B,QAAQ,OAAO,CAAE0B;oBACnB;oBAGA,IAAIL,AAAe,mBAAfA,MAAM,IAAI,EAAqB;wBACjCL,WAAWL,KAAK,GAAG,KAAKD;wBACxB,MAAMmC,iBAAiBxB,MAAM,KAAK;wBAGlC,MAAMS,aAAkC;4BACtC,SAAS;4BACThB;4BACA,mBAAmB;4BACnB,YAAY;4BACZ,OAAO+B,iBACH;gCACE,eAAeA,eAAe,YAAY,IAAI;gCAC9C,mBAAmBA,eAAe,aAAa,IAAI;gCACnD,cACGA,AAAAA,CAAAA,eAAe,YAAY,IAAI,KAC/BA,CAAAA,eAAe,aAAa,IAAI;gCACnC,WAAW7B,YAAY;gCACvB,YAAYtD;gCACZ,mBAAmBc;gCACnB,QAAQjB,iBAAiB,MAAM;4BACjC,IACAoE;wBACN;wBACA3B,QAAQ,OAAO,CAAE8B;wBACjB;oBACF;gBACF;gBACAjB,UAAUC;YACZ,OAAO;gBACL,MAAMoB,SAAS,MAAMjC,WAAW,MAAM,CAAC;oBACrC,OAAOvC;oBACP,QAAQ;oBACR,UAAUqC,SAAS,GAAG,CAAC,CAAC2C,IAAO;4BAC7B,MAAM;4BACN,SAASC,MAAM,OAAO,CAACD,EAAE,OAAO,IAC3BA,EAAE,OAAO,CAAS,GAAG,CAACL,uBACvBK,EAAE,OAAO;wBACf;oBACA,iBAAiBvC;oBACjB,GAAGc,YAAY;gBACjB;gBACAD,WAAWL,KAAK,GAAG,KAAKD;gBACxBG,UAAWqB,OAAe,OAAO,CAAC,EAAE,CAAC,IAAI;gBACzCnB,QAAQmB,OAAO,KAAK;YACtB;YAEAhD,OAAO2B,SAAS;QAClB;QAEA,IAAID,eAAe,CAACG,OAAO;YAEzB,MAAMa,kBAAkBC,KAAK,GAAG,CAC9B,GACAA,KAAK,KAAK,CAAEhB,AAAAA,CAAAA,WAAW,EAAC,EAAG,MAAM,GAAG;YAEtCE,QAAQ;gBACN,eAAea;gBACf,mBAAmBA;gBACnB,cAAcA,AAAkB,IAAlBA;YAChB;QACF;QAEA,OAAO;YACL,SAASf,WAAW;YACpB,OAAOE,QACH;gBACE,eAAeA,MAAM,aAAa,IAAI;gBACtC,mBAAmBA,MAAM,iBAAiB,IAAI;gBAC9C,cAAcA,MAAM,YAAY,IAAI;gBACpC,WAAWC,YAAY;gBACvB,YAAYtD;gBACZ,mBAAmBc;gBACnB,QAAQjB,iBAAiB,MAAM;YACjC,IACAoE;YACJ,YAAY,CAAC,CAACf;QAChB;IACF,EAAE,OAAOkC,GAAQ;QACfnD,QAAQ,KAAK,CAAC,kBAAkBmD;QAChC,MAAMC,WAAW,IAAIrD,MACnB,CAAC,eAAe,EAAEkB,cAAc,eAAe,GAAG,kBAAkB,EAAEkC,EAAE,OAAO,CAAC,8DAA8D,CAAC,EAC/I;YACE,OAAOA;QACT;QAEF,MAAMC;IACR;AACF;AAEO,MAAM3C,oBAAoB,CAC/B1C,WACAJ;IAIA,IAAI6C;IAKJ,IAAIzC,UAAU,QAAQ,CAAC,UACrB,OAAQJ;QACN,KAAK0F,aAAa,MAAM;YACtB7C,iBAAiB8C;YACjB;QACF,KAAKD,aAAa,eAAe;YAC/B7C,iBAAiB+C;YACjB;QACF,KAAKF,aAAa,IAAI;YACpB7C,iBAAiBgD;YACjB;QACF,KAAKH,aAAa,YAAY;QAC9B,KAAKA,aAAa,gBAAgB;YAChC7C,iBAAiB;gBAAE,MAAMiD,iBAAiB,IAAI;YAAC;YAC/C;IACJ;IAIF,IAAI1F,AAAc,wBAAdA,WACFyC,iBAAiB;QAAE,MAAMiD,iBAAiB,IAAI;IAAC;IAGjD,OAAOjD;AACT;AAEO,eAAekD,oBACpBtD,QAAsC,EACtCzC,iBAA+B,EAC/BC,gBAAmC;IAEnC,MAAM+F,WAAW,MAAMxD,KAAKC,UAAUzC,mBAAmBC;IACzD2B,OAAOoE,UAAU;IACjB,MAAMC,cAAcC,cAAcF,SAAS,OAAO,EAAE/F;IACpD,OAAO;QAAE,SAASgG;QAAa,OAAOD,SAAS,KAAK;IAAC;AACvD;AAEO,eAAeG,2BACpBC,IAAY,EACZpG,iBAA+B,EAC/BC,gBAAmC;IAEnC,MAAM,EAAEsD,OAAO,EAAEE,KAAK,EAAE,GAAG,MAAMjB,KAC/B4D,MACApG,mBACAC;IAEF,OAAO;QAAEsD;QAASE;IAAM;AAC1B;AAEO,SAAS4C,yBAAyBL,QAAgB;IACvD,IAAI;QAEF,MAAMM,YAAYN,SAAS,KAAK,CAAC;QACjC,IAAIM,WACF,OAAOA,SAAS,CAAC,EAAE;QAIrB,MAAMC,iBAAiBP,SAAS,KAAK,CACnC;QAEF,IAAIO,gBACF,OAAOA,cAAc,CAAC,EAAE;QAI1B,MAAMC,gBAAgBR,SAAS,KAAK,CAAC;QACrC,IAAIQ,eACF,OAAOA,aAAa,CAAC,EAAE;IAE3B,EAAE,OAAM,CAAC;IAET,OAAOR;AACT;AAEO,SAASS,yBAAyBC,KAAa;IACpD,IAAIA,MAAM,QAAQ,CAAC,SAEjB,MAAO,YAAY,IAAI,CAACA,OACtBA,QAAQA,MAAM,OAAO,CAAC,kBAAkB;IAG5C,OAAOA;AACT;AAEO,SAASR,cACdQ,KAAa,EACbzG,gBAAmC;IAEnC,MAAM0G,kBAAkBN,yBAAyBK;IAEjD,IAAIC,QAAAA,kBAAAA,KAAAA,IAAAA,gBAAiB,KAAK,CAAC,oBAAoB;YACtCC;QAAP,OAAO,QAAAA,CAAAA,yBAAAA,gBACJ,KAAK,CAAC,kBAAiB,IADnBA,KAAAA,IAAAA,uBAEH,KAAK,CAAC,GACP,GAAG,CAAC/C;IACT;IACA,IAAI;QACF,OAAOiB,KAAK,KAAK,CAAC6B;IACpB,EAAE,OAAM,CAAC;IACT,IAAI;QACF,OAAO7B,KAAK,KAAK,CAAC+B,WAAWF;IAC/B,EAAE,OAAOnB,GAAG,CAAC;IAEb,IACE5B,AAAmC,oBAAnCA,aAAa3D,qBACb2D,AAAmC,kBAAnCA,aAAa3D,mBACb;QACA,MAAM6G,aAAaL,yBAAyBE;QAC5C,OAAO7B,KAAK,KAAK,CAAC+B,WAAWC;IAC/B;IACA,MAAM1E,MAAM,CAAC,+BAA+B,EAAEsE,OAAO;AACvD"}