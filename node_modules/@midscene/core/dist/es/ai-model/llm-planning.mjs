import { vlLocateMode } from "@midscene/shared/env";
import { paddingToMatchBlockByBase64 } from "@midscene/shared/img";
import { getDebug } from "@midscene/shared/logger";
import { assert } from "@midscene/shared/utils";
import { AIActionType, buildYamlFlowFromPlans, callAiFn, fillBboxParam, findAllMidsceneLocatorField, markupImageForLLM, warnGPT4oSizeLimit } from "./common.mjs";
import { automationUserPrompt, generateTaskBackgroundContext, systemPromptToTaskPlanning } from "./prompt/llm-planning.mjs";
import { describeUserPage } from "./prompt/util.mjs";
const debug = getDebug('planning');
async function plan(userInstruction, opts) {
    var _planFromAI_action;
    const { callAI, context } = opts || {};
    const { screenshotBase64, size } = context;
    const modelPreferences = {
        intent: 'planning'
    };
    const { description: pageDescription, elementById } = await describeUserPage(context, modelPreferences);
    const systemPrompt = await systemPromptToTaskPlanning({
        actionSpace: opts.actionSpace,
        vlMode: vlLocateMode(modelPreferences)
    });
    const taskBackgroundContextText = generateTaskBackgroundContext(userInstruction, opts.log, opts.actionContext);
    const userInstructionPrompt = await automationUserPrompt(vlLocateMode(modelPreferences)).format({
        pageDescription,
        taskBackgroundContext: taskBackgroundContextText
    });
    let imagePayload = screenshotBase64;
    if ('qwen-vl' === vlLocateMode(modelPreferences)) imagePayload = await paddingToMatchBlockByBase64(imagePayload);
    else if (!vlLocateMode(modelPreferences)) imagePayload = await markupImageForLLM(screenshotBase64, context.tree, context.size);
    warnGPT4oSizeLimit(size, modelPreferences);
    const msgs = [
        {
            role: 'system',
            content: systemPrompt
        },
        {
            role: 'user',
            content: [
                {
                    type: 'image_url',
                    image_url: {
                        url: imagePayload,
                        detail: 'high'
                    }
                },
                {
                    type: 'text',
                    text: userInstructionPrompt
                }
            ]
        }
    ];
    const call = callAI || callAiFn;
    const { content, usage } = await call(msgs, AIActionType.PLAN, modelPreferences);
    const rawResponse = JSON.stringify(content, void 0, 2);
    const planFromAI = content;
    const actions = ((null == (_planFromAI_action = planFromAI.action) ? void 0 : _planFromAI_action.type) ? [
        planFromAI.action
    ] : planFromAI.actions) || [];
    const returnValue = {
        ...planFromAI,
        actions,
        rawResponse,
        usage,
        yamlFlow: buildYamlFlowFromPlans(actions, opts.actionSpace, planFromAI.sleep)
    };
    assert(planFromAI, "can't get plans from AI");
    actions.forEach((action)=>{
        const type = action.type;
        const actionInActionSpace = opts.actionSpace.find((action)=>action.name === type);
        const locateFields = actionInActionSpace ? findAllMidsceneLocatorField(actionInActionSpace.paramSchema) : [];
        debug('locateFields', locateFields);
        locateFields.forEach((field)=>{
            const locateResult = action.param[field];
            if (locateResult) if (vlLocateMode(modelPreferences)) action.param[field] = fillBboxParam(locateResult, size.width, size.height, modelPreferences);
            else {
                const element = elementById(locateResult);
                if (element) action.param[field].id = element.id;
            }
        });
    });
    assert(!planFromAI.error, `Failed to plan actions: ${planFromAI.error}`);
    if (0 === actions.length && returnValue.more_actions_needed_by_instruction && !returnValue.sleep) console.warn('No actions planned for the prompt, but model said more actions are needed:', userInstruction);
    return returnValue;
}
export { plan };

//# sourceMappingURL=llm-planning.mjs.map